---
title: "Algoritmo de Metropolis-Hastings: ejemplo unidimensional"
output:
  html_document:
    theme:
      version: 4
---

```{r replicabilidad, include=FALSE}
set.seed(23059823)
```

Supongamos que queremos estimar \( \mu = \mathbb{E}_{f}[X] \), donde \( f(x) \propto f_{u}(x) = x^{2} \mathrm{e}^{-x^{2} + \sin(x)} \).

La aplicación del método de Montecarlo para realizar esa estimación exige que generemos valores aleatorios distribuidos según la densidad \( f \) (que es \( f_{u} \) multiplicada por una constante de normalización adecuada que hace que su integral valga 1). Puesto que no se trata de una densidad estándar, usaremos el algoritmo de Metropolis-Hastings para construir una cadena de Markov cuya distribución límite sea \(f\).

Vamos a aplicar el algoritmo de dos maneras distintas: en primer lugar, mediante un muestreador independiente; en segundo lugar, mediante un paseo aleatorio haciendo uso del paquete `mcmc`. 


## Algoritmo de Metropolis-Hastings con un muestreador independiente

Con el objetivo de elegir un muestreador independiente adecuado, procedemos a representar gráficamente la densidad no normalizada \( f_{u} \).

```{r densidad-objetivo}
library(ggplot2)

f_u <- function(x) {
  x^2 * exp(-x^2 + sin(x))
}

ggplot() +
  geom_function(fun = f_u) +
  xlim(-4, 4)
```

Puesto que \( f_{u} \) es positiva en todo \( \mathbb{R} \) (excepto en el único punto \( x = 0 \)), el muestreador \( q \) que elijamos debe poder proponer como nuevo estado cualquier número real (es decir, también debe ser positiva en todo \( \mathbb{R} \)). Un candidato natural es usar una distribución normal \( N(\mu, \sigma^{2}) \). Por otra parte, la gráfica anterior muestra que los valores más probables son los del intervalo \( (-2, 2) \), con el máximo alcanzándose en aproximadamente \(x = 1\). Por tanto, unos parámetros adecuados podrían ser \( \mu = 1, \sigma = 1 \), tal y como se observa en la siguiente gráfica.

```{r densidad-instrumental}
q <- function(y) {
  dnorm(y, mean = 1, sd = 1)
}

ggplot() +
  geom_function(fun = f_u) +
  geom_function(fun = q, colour = "red") +
  xlim(-4, 4)
```

El siguiente código genera una realización de longitud 10 de la cadena de Markov construida por el algoritmo de Metropolis-Hastings, mostrando con todo detalle el funcionamiento de este último.

```{r algoritmo-Metropolis-Hastings}
n <- 10
estado_actual <- 1
for (t in seq_len(n - 1)) {
  print(sprintf("Estado actual: %.3f", estado_actual))
  estado_propuesto <- rnorm(1, mean = 1, sd = 1)
  razon_Hastings <-
    (f_u(estado_propuesto) * q(estado_actual)) /
      (f_u(estado_actual) * q(estado_propuesto))
  u <- runif(1)
  print(sprintf(
    "   Estado propuesto: %.3f, Prob. de aceptar: %.3f, %s (u = %.3f)",
    estado_propuesto,
    min(1, razon_Hastings),
    if (u < razon_Hastings) {
      "Aceptar"
    } else {
      "Rechazar"
    },
    u
  ))
  if (u < razon_Hastings) {
    estado_actual <- estado_propuesto
  }
}
```

Obsérvese que la densidad instrumental escogida garantiza que es posible alcanzar en un paso cualquier \( y \in \mathbb{R} \setminus \{0\} \), ya que \( q(y) > 0 \), luego es posible que se proponga \( y \) como nuevo estado, y la razón de Hastings no es nula, luego es posible que se acepte \( y \) como nuevo estado. Esto quiere decir que la cadena de Markov construida es \( f \)-irreducible.

La cadena de Markov construida es aperiódica, ya que se cumple la condición suficiente de que la probabilidad de rechazar el nuevo estado propuesto sea mayor que cero. La teoría de cadenas de Markov asegura entonces que la cadena de Markov construida por el algoritmo tiene a \( f \) como distribución límite. Podemos comprobarlo generando muchas realizaciones de esa cadena y comprobando que la distribución de los últimos estados de esas realizaciones se corresponde con \( f \) (el histograma y la gráfica de la función \( f_{u} \) no coinciden exactamente porque recordemos que \( f_{u} \) no está normalizada).

```{r distribución-limite}
n <- 1e3
m <- 1e3
valores <- replicate(m, {
  estado_actual <- rnorm(1, mean = 1, sd = 1)
  for (t in seq_len(n - 1)) {
    estado_propuesto <- rnorm(1, mean = 1, sd = 1)
    razon_Hastings <-
      (f_u(estado_propuesto) * q(estado_actual)) /
        (f_u(estado_actual) * q(estado_propuesto))
    u <- runif(1)
    if (u < razon_Hastings) {
      estado_actual <- estado_propuesto
    }
  }
  estado_actual
})

ggplot(data.frame(x = valores), aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = .25) +
  geom_function(fun = f_u)
```

La práctica habitual es, sin embargo, generar una única realización de la cadena de Markov que proporcionará mejores estimaciones cuanto mayor longitud tenga.

```{r única-realización}
n <- 1e6
estados <- numeric(n)
estados[1] <- 1
for (t in seq_len(n - 1)) {
  estado_actual <- estados[t]
  estado_propuesto <- rnorm(1, mean = 1, sd = 1)
  razon_Hastings <-
    (f_u(estado_propuesto) * q(estado_actual)) /
      (f_u(estado_actual) * q(estado_propuesto))
  u <- runif(1)
  if (u < razon_Hastings) {
    estados[t + 1] <- estado_propuesto
  } else {
    estados[t + 1] <- estado_actual
  }
}

ggplot(data.frame(x = estados), aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = .25) +
  geom_function(fun = f_u)
```

Para estimar el valor de \( \mu \) basta entonces calcular la media de los estados incluidos en la realización obtenida de la cadena de Markov.

```{r estimación-independiente}
estimacion <- mean(estados)
estimacion
```

Como para todo resultado obtenido mediante el método de Montecarlo, es conveniente proporcionar un intervalo de confianza para el mismo. Hay que tener en cuenta que los estados de una cadena de Markov no son independientes entre sí, por lo que no se podría aplicar de manera directa el teorema central del límite para la construcción de esos intervalos. Una manera de solventar esta dificultad es mediante el método de las medias por lotes, que consiste en lo siguiente:

1. Separar los estados de la cadena de Markov en un cierto número de subgrupos de la misma longitud llamados lotes.
2. Calcular la media de los estados de cada lote. Se obtienen entonces unos valores que estarán más incorrelados cuanto mayor sea la longitud de los lotes.
3. Estimar el valor de \( \mu \) y construir un intervalo de confianza de la manera habitual a partir de los valores obtenidos en el paso anterior. La estimación será más precisa cuanto más valores se disponga.

Atendiendo a lo indicado en los puntos 2 y 3, si la cantidad total de estados a generar de la cadena de Markov está prefijado de antemano, entonces hay que buscar un equilibrio entre que la cantidad de lotes sea suficiente y que estos sean lo suficientemente largos.

```{r medias-por-lotes}
numero_lotes <- 500
longitud_lotes <- n / numero_lotes
medias_lotes <- numeric(numero_lotes)
for (i in seq_len(numero_lotes)) {
  indices_lotes <- seq(
    longitud_lotes * (i - 1) + 1,
    longitud_lotes * i
  )
  medias_lotes[i] <- mean(estados[indices_lotes])
}
estimacion <- mean(medias_lotes)
error_estandar <- sqrt(var(medias_lotes) / numero_lotes)
probabilidad_cobertura <- 0.95
alfa <- 1 - probabilidad_cobertura
percentil <- qnorm(1 - alfa / 2)
intervalo_confianza <- estimacion + c(-1, 1) * error_estandar * percentil
```

Se obtiene entonces para \( \mu \) una estimación de `r estimacion`, con un error estándar de `r error_estandar` y (`r intervalo_confianza`) un intervalo de confianza con probabilidad de cobertura `r probabilidad_cobertura`.

Podemos comprobar a través de un gráfico de autocorrelación que las medias de los lotes son, efectivamente, prácticamente independientes entre sí.

```{r autocorrelación-lotes}
library(coda)

autocorr.plot(mcmc(medias_lotes))
```

Esto no ocurre con los estados de la cadena de Markov, por lo que la estimación del error estándar a partir de la varianza muestral subestimaría el valor del primero al no tener en cuenta esta última las correlaciones entre los datos.

```{r autocorrelación-estados}
autocorr.plot(mcmc(estados))
sqrt(var(estados) / n)
```


## Paseo aleatorio con el paquete `mcmc`

La función `metrop` del paquete `mcmc` implementa un paseo aleatorio de Metropolis. Es decir, el nuevo estado propuesto por el algoritmo viene dado por
\[
  Y = X_{t} + E, \qquad \text{con } E \sim \mathrm{N}(0, \Sigma)
\]

La función admite los siguientes argumentos:

* Una función que calcula el logaritmo de la densidad (posiblemente no normalizada) de la distribución estacionaria deseada para la cadena de Markov. Esta función debe devolver `-Inf` para todos aquellos argumentos en los que la densidad devolvería el valor cero.
* `initial`: el estado inicial de la cadena de Markov.
* `nbatch`: el número de lotes (o de estados, si la longitud de cada lote es 1) requeridos.
* `blen`: la longitud de cada lote (por defecto, 1).
* `scale`: determina \( \Sigma \), como `scale %*% t(scale)`. Puede ser también un escalar o un vector, en cuyo caso se toma la matriz diagonal correspondiente. El valor por defecto es 1.

```{r paseo-aleatorio-Metropolis}
library(mcmc)

log_f_u <- function(x) {
  if (x == 0) {
    -Inf
  } else {
    2 * log(abs(x)) - x^2 + sin(x)
  }
}

paseo_Metropolis <- metrop(log_f_u, initial = 1, nbatch = 1e4)
```

La función `metrop` devuelve una lista con mucha información acerca de la cadena construida. En concreto, la componente `batch` contiene los estados de la cadena de Markov (en realidad, contiene las medias de los lotes, pero en este caso coinciden porque por defecto los lotes son de longitud 1).

```{r salida-metrop}
str(paseo_Metropolis)
head(paseo_Metropolis$batch)
```

Es habitual realizar un diagnóstico de convergencia mediante un gráfico que muestra la evolución de los estados de la cadena de Markov a lo largo del tiempo. Este tipo de gráficos nunca se puede usar para asegurar la convergencia, ya que aunque no muestre signos obvios de no convergencia no es posible saber qué podría ocurrir en instantes futuros.

```{r traza-cadena-Markov}
traceplot(mcmc(paseo_Metropolis$batch))
```

En este caso, este gráfico de densidad proporciona una confianza adicional en la convergencia de la cadena de Markov.

```{r gráfico-densidad-cadena-Markov}
densplot(mcmc(paseo_Metropolis$batch))
```

Para intentar que la cadena de Markov converja lo más rápido posible, hay una regla empírica obtenida a partir del análisis de un problema concreto (y que, por lo tanto, no siempre es aplicable) que establece que el porcentaje de nuevos estados aceptados debería ser del 25 %. El razonamiento es que un porcentaje demasiado bajo implica que la cadena se encuentre estancada la mayor parte del tiempo, mientras que un porcentaje demasiado alto indicará que los nuevos estados propuestos varían muy poco con respecto a los estados actuales y, por tanto, la cadena de Markov evoluciona muy lentamente.

La componente `accept` de la salida de la función `metrop` proporciona ese porcentaje.

```{r porcentaje-estados-aceptados}
paseo_Metropolis$accept
```

La función `metrop` tiene la capacidad de «continuar extendiendo» la cadena de Markov cambiando o manteniendo los valores de los parámetros. Para ello basta proporcionarle como argumentos la salida anterior y los nuevos valores de los parámetros que cambien. Para aumentar el porcentaje de nuevos estados aceptados se puede reducir el valor de `scale`, ya que eso hará que se propongan nuevos estados «más cercanos» a los estados actuales y, en consecuencia, con mayor probabilidad de ser aceptados. Por el razonamiento contrario, para disminuir ese porcentaje bastará aumentar el valor de `scale`.

```{r 25%-estados-aceptados}
paseo_Metropolis <- metrop(paseo_Metropolis, scale = 2)
paseo_Metropolis$accept
paseo_Metropolis <- metrop(paseo_Metropolis, scale = 3)
paseo_Metropolis$accept
```

Finalmente, aplicamos el método de las medias por lotes para obtener una estimación de \( \mu \) y un intervalo de confianza. El gráfico de autocorrelación nos indica que lotes de longitud 20 serían suficiente para obtener valores poco correlados.

```{r autocorrelación-paseo-Metropolis}
autocorr.plot(mcmc(paseo_Metropolis$batch))
```

Para mayor seguridad, y para poder compararlo con el resultado del muestreador independiente, consideraremos lotes de longitud 2000.

```{r medias-por-lotes-paseo-Metropolis}
paseo_Metropolis <- metrop(paseo_Metropolis, nbatch = 500, blen = 2000)
autocorr.plot(mcmc(paseo_Metropolis$batch))

library(purrr)

estimacion <- paseo_Metropolis$batch |> 
  array_branch(margin = 2) |> 
  map_dbl(mean)
error_estandar <- paseo_Metropolis$batch |> 
  array_branch(margin = 2) |> 
  map_dbl(var) |> 
  magrittr::divide_by(paseo_Metropolis$nbatch) |> 
  sqrt()

probabilidad_cobertura <- 0.95
alfa <- 1 - probabilidad_cobertura
percentil <- qnorm(1 - alfa / 2)
intervalo_confianza <- estimacion + c(-1, 1) * error_estandar * percentil
```

Se obtiene entonces para \( \mu \) una estimación de `r estimacion`, con un error estándar de `r error_estandar` y (`r intervalo_confianza`) un intervalo de confianza con probabilidad de cobertura `r probabilidad_cobertura`.


## Algoritmo de Metropolis-Hastings con un muestreador independiente inadecuado

Supongamos que pretendemos aplicar el algoritmo de Metropolis-Hastings tomando como muestreador independiente la distribución exponencial de parámetro 1. Como la función de densidad de esta distribución solo es positiva para los números no negativos, la cadena de Markov obtenida no será irreducible y no convergerá a la distribución pretendida.

```{r densidad-instrumental-incorrecta}
log_q <- function(y) {
  dexp(y, rate = 1, log = TRUE)
}
```

Por ejemplo, si el estado inicial es un número positivo, la cadena de Markov no será capaz de alcanzar la región de números negativos.

```{r reducible-inicial-positivo}
n <- 1e4
estados <- numeric(n)
estados[1] <- 1
for (t in seq_len(n - 1)) {
  estado_actual <- estados[t]
  estado_propuesto <- rexp(1, rate = 1)
  log_razon_Hastings <-
    (log_f_u(estado_propuesto) + log_q(estado_actual)) -
    (log_f_u(estado_actual) + log_q(estado_propuesto))
  log_u <- log(runif(1))
  if (log_u < log_razon_Hastings) {
    estados[t + 1] <- estado_propuesto
  } else {
    estados[t + 1] <- estado_actual
  }
}

traceplot(mcmc(estados))
```

Por el contrario, si el estado inicial es un número negativo, la cadena de Markov será constante, ya que se rechazarán todos los nuevos estados propuestos al ser cero la densidad instrumental para el estado actual y, en consecuencia, cero la razón de Hastings para el estado propuesto.

```{r reducible-inicial-negativo}
n <- 1e4
estados <- numeric(n)
estados[1] <- -1
for (t in seq_len(n - 1)) {
  estado_actual <- estados[t]
  estado_propuesto <- rexp(1, rate = 1)
  log_razon_Hastings <-
    (log_f_u(estado_propuesto) + log_q(estado_actual)) -
    (log_f_u(estado_actual) + log_q(estado_propuesto))
  log_u <- log(runif(1))
  if (log_u < log_razon_Hastings) {
    estados[t + 1] <- estado_propuesto
  } else {
    estados[t + 1] <- estado_actual
  }
}

traceplot(mcmc(estados))
```
